# -*- coding: utf-8 -*-
"""Copy of Titanic Advanced ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d9gmjZ1hBYZA1GegL8qA0qDpeQdXmXEe
"""

import pandas as pd
import numpy as np

"""###Loading Dataset + Data Exploration"""

df = pd.read_csv('/content/drive/MyDrive/Machine Learning/Advanced ML/Data/Titanic_train.csv')

df.head()

df.columns

df.describe()

df.dtypes

df['Embarked']

"""###Cleaning Dataset"""

df.isna().sum()

#dropping the embarked missing values

df = df.dropna(subset=['Embarked'])

#Checking if rows were dropped

df.isna().sum()

# Extracting title from name

df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)

# Geting median age per title and class

age_medians = df.groupby(['Title', 'Pclass'])['Age'].median()

# Fill missing ages based on titles and classes

df['Age'] = df.groupby(['Title', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))

#Checking if missing values were filled

df.isna().sum()

df['Cabin'].isna().describe()

#creating 'has_cabin' feature to indicate whether missing values impact data.
#.notna().astype(int) will ensure that missing values = 0 and existing values = 1

df['has_cabin'] = df['Cabin'].notna().astype(int)

#checking if missing values are there for 'has_cabin

df['has_cabin'].isna().sum()

#now that we have the 'has_cabin' column, let's check correlations to see whether it's important to invest in cleaning these rows well.

df['has_cabin'].corr(df['Survived'])

"""since 77% of data within the "Cabin" column is missing, any attempt at imputation is largely guesswork. It's in our best interest to run with the 'has_cabin' column since it moderately correlates with "Survived."

Now...we'll start encoding.
"""

df.dtypes

"""Need to encode 'Title', 'Sex', 'Ticket', and 'Embarked'"""

title_mapping = {
   'Mr': 1,       # Adult male
   'Mrs': 2,      # Married female
   'Miss': 3,     # Unmarried female
   'Master': 4,   # Young male child
   'Dr': 5,
   'Rev': 5,
   'Col': 5,
   'Major': 5,
   'Mlle': 3,     # Unmarried female (French)
   'Ms': 3,       # Female unknown marriage status
   'Lady': 2,
   'Sir': 1,
   'Mme': 2,      # Married female (French)
   'Don': 1,
   'Dona': 2,
   'Capt': 5,
   'Jonkheer': 5,
   'Countess': 2
}

df['Title_encoded'] = df['Title'].map(title_mapping)

#Verifying that title column has been encoded

df['Title_encoded']

#Checking whether male and female are only entries

df['Sex'].describe()

sex_entries = {'male': 1, 'female': 0}

df['Sex_encoded'] = df['Sex'].map(sex_entries)

#Verifying that sex column has been encoded


df['Sex_encoded']

df['Ticket']

"""I've decided not to encode 'Ticket' column since it likely has no correlation with Survived"""

df['Embarked'].unique()

embarked_entries = {'S': 0, 'C': 1, 'Q': 2}

df['Embarked_encoded'] = df['Embarked'].map(embarked_entries)

df['Embarked_encoded']

"""###Baseline ML

Random Forrest
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

df.columns

#defining model features. Didn't include "Survived" because that is the label. Didn't include "Cabin" because of missing values. Didn't include 'Name'

features = ['PassengerId', 'Pclass', 'Sex_encoded', 'Age', 'SibSp',
       'Parch', 'Fare', 'Embarked_encoded', 'Title_encoded', 'has_cabin']

X = df[features]

y = df['Survived']

random_forest_model = RandomForestClassifier(n_estimators=100, random_state=2)

# Assuming X is your features and y is your target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

random_forest_model.fit(X_train, y_train)

y_pred = random_forest_model.predict(X_test)
y_prep = random_forest_model.fit(X_train, y_train)
accuracy_score(y_test, y_pred)

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

logreg.fit(X_train,y_train)

y_pred = logreg.predict(X_test)

accuracy_score(y_test,y_pred)

"""###Attempt at improving Random Forrest"""

import numpy as np
from sklearn.model_selection import RandomizedSearchCV

param_distributions = {
    'n_estimators': np.arange(100, 1000, 100),
    'max_depth': np.arange(3, 30),
    'min_samples_split': np.arange(2, 20),
    'min_samples_leaf': np.arange(1, 10),
    'max_features': ['sqrt', 'log2']
}


rf = RandomForestClassifier(random_state=42)

random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_distributions,
    n_iter=100,  # number of parameter settings sampled
    cv=5,        # 5-fold cross-validation
    random_state=42,
    n_jobs=-1)

random_search.fit(X, y)

print("Best parameters:", random_search.best_params_)
print("Best cross-validation score:", random_search.best_score_)

new_rf = RandomForestClassifier(
    n_estimators=100,
    min_samples_split=15,
    min_samples_leaf=3,
    max_features='log2',
    max_depth=19,
    random_state=42)

new_rf.fit(X_train, y_train)

y_pred = new_rf.predict(X_test)

print("Test set accuracy:", accuracy_score(y_test, y_pred))

"""###Test Dataset to test validation"""

test = pd.read_csv('/content/drive/MyDrive/Machine Learning/Advanced ML/Data/Titanic_test.csv')

test.head()

"""Cleaning Test Dataset"""

test.isna().sum()

#dropping the fare missing values

test = test.dropna(subset=['Fare'])

#Verifying that missing values for 'Fare' were dropped

test['Fare'].isna().sum()

# Extracting title from name

test['Title'] = test['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)

# Getting median age per title and class

age_medians = test.groupby(['Title', 'Pclass'])['Age'].median()

# Fill missing ages based on titles and classes

test['Age'] = test.groupby(['Title', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))

#Verifying that 'Age' column had its rows w/ missing data filled. We got all plus one row...we'll drop that one.

test['Age'].isna().sum()

#Dropping last missing value

test = test.dropna(subset=['Age'])

#great, now 'age' column is done!

test['Age'].isna().sum()

#creating 'has_cabin' feature as was done with train dataset.
#.notna().astype(int) will ensure that missing values = 0 and existing values = 1

test['has_cabin'] = test['Cabin'].notna().astype(int)

#checking if missing values are there for 'has_cabin

test['has_cabin'].isna().sum()

# All features that take part in the model--ie, not 'Cabin'--have been cleaned
# from missing values

test.isna().sum()

"""Encoding Test Dataset"""

title_mapping = {
   'Mr': 1,       # Adult male
   'Mrs': 2,      # Married female
   'Miss': 3,     # Unmarried female
   'Master': 4,   # Young male child
   'Dr': 5,
   'Rev': 5,
   'Col': 5,
   'Major': 5,
   'Mlle': 3,     # Unmarried female (French)
   'Ms': 3,       # Female unknown marriage status
   'Lady': 2,
   'Sir': 1,
   'Mme': 2,      # Married female (French)
   'Don': 1,
   'Dona': 2,
   'Capt': 5,
   'Jonkheer': 5,
   'Countess': 2
}

test['Title_encoded'] = test['Title'].map(title_mapping)

test['Title_encoded']

sex_entries = {'male': 1, 'female': 0}

test['Sex_encoded'] = test['Sex'].map(sex_entries)

test['Sex_encoded']

embarked_entries = {'S': 0, 'C': 1, 'Q': 2}

test['Embarked_encoded'] = test['Embarked'].map(embarked_entries)

test['Embarked_encoded']

"""Now fitting the test dataset to the RF model"""

features_test = features = ['PassengerId', 'Pclass', 'Sex_encoded', 'Age', 'SibSp',
       'Parch', 'Fare', 'Embarked_encoded', 'Title_encoded', 'has_cabin']

test.columns

x = test[features_test]

new_rf = RandomForestClassifier(
    n_estimators=100,
    min_samples_split=15,
    min_samples_leaf=3,
    max_features='log2',
    max_depth=19,
    random_state=42)

new_rf.fit(X, y)

y_pred_tests = new_rf.predict(x)

"""###Kaggle Submission"""

# Creating Kaggle DataFrame
Kaggle = pd.DataFrame({
    'PassengerId': test['PassengerId'],  # Using your test DataFrame
    'Survived': y_pred_tests
})

# Save to CSV
Kaggle.to_csv('/content/drive/MyDrive/Machine Learning/Advanced ML/Data/titanic_KaggleSubmission.csv', index=False)

kaggle

# Check if there are any missing PassengerIds
print("Missing PassengerIds:", kaggle['PassengerId'].isna().sum())

# Check the range of PassengerIds to see if we skipped any
print("Min PassengerId:", kaggle['PassengerId'].min())
print("Max PassengerId:", kaggle['PassengerId'].max())

# Look at the PassengerIds in sequence to find gaps
print("PassengerIds not in sequence:",
      set(range(kaggle['PassengerId'].min(),
                kaggle['PassengerId'].max() + 1)) - set(kaggle['PassengerId']))

# Check original test data
print("Test dataset shape:", test.shape)
print("\nFirst few PassengerIds in test:", test['PassengerId'].head())
print("Last few PassengerIds in test:", test['PassengerId'].tail())

# Check our kaggle
print("\nkaggle shape:", kaggle.shape)
print("\nFirst few PassengerIds in kaggle:", kaggle['PassengerId'].head())
print("Last few PassengerIds in kaggle:", kaggle['PassengerId'].tail())

