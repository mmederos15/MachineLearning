# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mGSBNvd18EygSIm3j3Of4FBYMGpUqAjR
"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

x = torch.linspace(-2*torch.pi,2*torch.pi,1000).unsqueeze(1)
y_true = torch.sin(x)

model = nn.Sequential(
    nn.Linear(1,64),
    nn.Tanh(),
    nn.Linear(64,64),
    nn.Tanh(),
    nn.Linear(64,1)
)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr = 0.01)

for i in range(300):
  y_pred = model(x)
  loss = criterion(y_pred, y_true)

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

with torch.no_grad():
    y_pred = model(x)

plt.plot(x.numpy(), y_true.numpy(), label='True sin(x)')
plt.plot(x.numpy(), y_pred.numpy(), label='Model', color='red')
plt.legend()
plt.title("Neural Network Fit to sin(x)")
plt.show()

text = "the quick brown fox jumped over the lazy dog"
chars = sorted(list(set(text)))

stoi = {ch: i for i,ch in enumerate(chars)}
itos = {i:ch for i, ch in stoi.items()}

def encode(s):
  return [stoi[c] for c in s]

data = torch.tensor(encode(text), dtype=torch.long)
data.shape

context_size = 2
vocab_size = len(stoi)
X=[]
Y=[]

for i in range(len(data) - context_size):
  X.append(data[i:i+context_size])
  Y.append(data[i+context_size])
X = torch.stack(X)
Y = torch.tensor(Y)

model = nn.Sequential(
    nn.Embedding(vocab_size, 10),
    nn.Flatten(),
    nn.Linear(context_size * 10, 100),
    nn.ReLU(),
    nn.Linear(100,vocab_size)
)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr = 0.01)

for epoch in range(1000):
  logits = model(X)
  loss = criterion(logits, Y)

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

def sampling(model, start_text, length = 100):
  model.eval()
  context = torch.tensor(encode(start_text[-context_size:]), dtype=torch.long).unsqueeze(0)
  generated = start_text